{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de costos de cursos en el mercado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from hermetrics.levenshtein import Levenshtein\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rutas de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(os.path.join(os.getcwd(), '..','2_data_understanding_module','collecting_initial_data','existing_data','DIRTY_DATA'))\n",
    "path_additional_data = os.path.abspath(os.path.join(os.getcwd(), '..','2_data_understanding_module','collecting_initial_data','additional_data'))\n",
    "path_dwh = os.path.abspath(os.path.join(os.getcwd(),'DATAWAREHOUSE'))\n",
    "path_ppd = os.path.abspath(os.path.join(os.getcwd(),'CLEANED_DATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos de costos de educacion continua, costos de cursos cec y cursos de OC, OEC y CI\n",
    "\n",
    "df_dim_perfil = pd.read_csv(os.path.join(path_dwh,'datamart_oec','dim_perfil.csv'))\n",
    "df_dim_curso = pd.read_csv(os.path.join(path_dwh,'datamart_ci','dim_curso.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ['Python', 'programación web', 'paginas web', 'datos', 'Excel', 'Word', 'power point', 'BI', 'inteligencia de negocios', 'powerBI', \n",
    "'big data', 'programación', 'marketing', 'web', 'html', 'css',  'java', 'javascript', 'base de datos', 'redes', 'telecomunicaciones', 'Android', 'IOS', 'ARDUINO'\n",
    "'Business Intelligence', \n",
    "'Desarrollo de software',\n",
    "'Fibra Óptica',\n",
    "'Gerencia de sistemas',\n",
    "'Hackeo Ético',\n",
    "'Informática Forense',\n",
    "'Linux', 'WINDOWS'\n",
    "'Office',\n",
    "'PHP',\n",
    "'Project',\n",
    "'software R', 'Tableu', 'rapidminer', \n",
    "'Mysql', 'sql', 'sqlserver', 'Oracle', 'postgresql',\n",
    "'Seguridad informática',\n",
    "'Aulas virtuales', 'Moodle', 'software',\n",
    "'Wordpress', 'joomla', 'xoops', 'drupal'\n",
    "]\n",
    "\n",
    "keyword = np.char.lower(np.array(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev = Levenshtein()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar las areas de Tecnologia y TICs\n",
    "\n",
    "df_dim_curso_perfil = pd.read_csv(os.path.join(path_dwh,'datamart_oc','dim_curso_perfil.csv'))\n",
    "df_dim_curso_perfil = df_dim_curso_perfil[df_dim_curso_perfil['area_familia'] == 'TECNOLOGÍAS DE LA INFORMACIÓN Y COMUNICACIÓN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coec = pd.read_csv(os.path.join(path_ppd,'costos_cursos_educacion_continua.csv'))\n",
    "df_ccce = pd.read_csv(os.path.join(path_ppd,'costos_de_cursos_cec_epn.csv'))\n",
    "df_coec = pd.concat([df_coec,df_ccce])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= df_dim_curso_perfil.groupby(['curso_perfil','modalidad']).count().reset_index()\n",
    "data = data[['curso_perfil','modalidad']]\n",
    "data['curso_perfil'] = data['curso_perfil'].str.cat(data['modalidad'],sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "\n",
    "for i in range(len(df_coec['curso'])):\n",
    "    # Similaridad \n",
    "    sub_arr = []\n",
    "    for j in range(len(data['curso_perfil'])):\n",
    "        curso_fuente = df_coec['curso'].iloc[i]\n",
    "        curso_comparar = data['curso_perfil'].iloc[j]\n",
    "        similary = lev.similarity(curso_fuente,curso_comparar)\n",
    "        aux_similary=0\n",
    "        arr_curso = curso_comparar.split(' ')\n",
    "        if curso_comparar in curso_fuente :\n",
    "            similary = 1.0\n",
    "        else:\n",
    "            for word in arr_curso:\n",
    "                if word in curso_fuente:\n",
    "                    aux_similary+=1/len(arr_curso)\n",
    "        if aux_similary > similary and aux_similary > 2/len(arr_curso):\n",
    "            similary = aux_similary\n",
    "        sub_arr.append(similary)\n",
    "    arr.append(sub_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = open(os.path.join(path_ppd,'spanish.txt'),\"r\", encoding='utf-8')\n",
    "stopwords = stopwords.readlines()\n",
    "stopwords = [x.replace('\\n','') for x in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevencion manejo desechos hospitalarios\n"
     ]
    }
   ],
   "source": [
    "def normalizar(text):\n",
    "    puntuacion = '!\"#$%&;\\'()*+,-./?@[\\]^_`{|}~' \n",
    "    text = text.replace('Á','A')\n",
    "    text = text.replace('É','E')\n",
    "    text = text.replace('Í','I')\n",
    "    text = text.replace('Ó','O')\n",
    "    text = text.replace('Ú','U')\n",
    "    text = text.strip().lower()\n",
    "    text = text.split(' ')\n",
    "    re_punc = re.compile('[%s]' % re.escape(puntuacion))\n",
    "    stripped = [re_punc.sub(' ', w) for w in text]\n",
    "    text = ' '.join(stripped)\n",
    "    text = text.split(' ')\n",
    "    new_text = []\n",
    "    for i in range(len(text)):\n",
    "        char = text[i]\n",
    "        if char not in stopwords:\n",
    "            new_text.append(text[i])\n",
    "    new_text = ' '.join(new_text)\n",
    "            \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(data=arr,index=df_coec['curso'],columns=data['curso_perfil'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.208333333"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = res[res['EXCEL BÁSICO | ONLINE']==res['EXCEL BÁSICO | ONLINE'].max()]['EXCEL BÁSICO | ONLINE'].index.values[0]\n",
    "df_coec[df_coec['curso']==val]['precio_por_hora'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_curso_perfil['curso_perfil_modalidad'] = df_dim_curso_perfil['curso_perfil'].str.cat(df_dim_curso_perfil['modalidad'],sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_curso_fuente = []\n",
    "arr_precio_por_hora = []\n",
    "for i in range(df_dim_curso_perfil.shape[0]):\n",
    "    query = df_dim_curso_perfil['curso_perfil_modalidad'].iloc[i]\n",
    "    val = res[res[query]==res[query].max()][query].index.values[0]\n",
    "    arr_curso_fuente.append(df_coec[df_coec['curso']==val][['curso','precio_por_hora']].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 411 entries, 0 to 410\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   curso_fuente     411 non-null    object \n",
      " 1   precio_por_hora  411 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 6.5+ KB\n"
     ]
    }
   ],
   "source": [
    "aux_data = pd.DataFrame(data=arr_curso_fuente,columns=['curso_fuente','precio_por_hora'])\n",
    "aux_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = pd.DataFrame(data=[df_dim_curso_perfil['curso_perfil_modalidad'].values,aux_data['curso_fuente'].values,aux_data['precio_por_hora'].values])\n",
    "res2 = res2.transpose()\n",
    "res2 = res2.rename(columns={0:'curso_perfil_modalidad',1:'curso_fuente',2:'precio_por_hora'})\n",
    "#res2[res2['curso_fuente']=='PYTHON | PRESENCIAL']\n",
    "#res2[res2['curso_perfil_modalidad'].str.contains('BI')]\n",
    "#res2['curso_fuente'].value_counts()\n",
    "'python' in 'tpythont' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Realizar un procesado de texto en donde se contenga los nombres de los cursos por modalidad en caso de OC el resto de cursos solo se tomará un promedio general estimado de estos cursos\n",
    "\n",
    "# Procesado de texto:\n",
    "#   Aplicar a lista de las palabras clave de tecnologia\n",
    "#   Validar si un curso es igual a la lista de costos de cursos\n",
    "#   Buscar simetría en el nombre de los cursos en el sitio web y el nombre de los cursos de las listas de costos\n",
    "#   Revisará los nombre de todos los cursos y determinará las palabras sin palabras_de_parada que se repitan más para obtener palabras clave en cuenta a cursos para buscarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sjime/nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sjime\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sjime/nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sjime\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\src\\University\\Data-Mining-System-SETEC\\3_data_preprocesing_module\\ETL_Costs.ipynb Celda 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/University/Data-Mining-System-SETEC/3_data_preprocesing_module/ETL_Costs.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/University/Data-Mining-System-SETEC/3_data_preprocesing_module/ETL_Costs.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m stop_words \u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/University/Data-Mining-System-SETEC/3_data_preprocesing_module/ETL_Costs.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(stop_words)\n",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sjime\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sjime/nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\sjime\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sjime\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "857d8669872199a61137514e88186641c3493fcbfa068d2d6eeeca0b699193af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
